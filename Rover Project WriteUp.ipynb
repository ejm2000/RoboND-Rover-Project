{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Search and Sample Return by Edwin Montgomery\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**The goals / steps of this project are the following:**  \n",
    "\n",
    "**Training / Calibration**  \n",
    "\n",
    "* Download the simulator and take data in \"Training Mode\"\n",
    "* Test out the functions in the Jupyter Notebook provided\n",
    "* Add functions to detect obstacles and samples of interest (golden rocks)\n",
    "* Fill in the `process_image()` function with the appropriate image processing steps (perspective transform, color threshold etc.) to get from raw images to a map.  The `output_image` you create in this step should demonstrate that your mapping pipeline works.\n",
    "* Use `moviepy` to process the images in your saved dataset with the `process_image()` function.  Include the video you produce as part of your submission.\n",
    "\n",
    "**Autonomous Navigation / Mapping**\n",
    "\n",
    "* Fill in the `perception_step()` function within the `perception.py` script with the appropriate image processing functions to create a map and update `Rover()` data (similar to what you did with `process_image()` in the notebook). \n",
    "* Fill in the `decision_step()` function within the `decision.py` script with conditional statements that take into consideration the outputs of the `perception_step()` in deciding how to issue throttle, brake and steering commands. \n",
    "* Iterate on your perception and decision function until your rover does a reasonable (need to define metric) job of navigating and mapping.  \n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./misc/rover_image.jpg\n",
    "[image2]: ./calibration_images/example_grid1.jpg\n",
    "[image3]: ./calibration_images/example_rock1.jpg \n",
    "[image4]: ./code/autonomous_mapping.png\n",
    "[image5]: ./misc/navigable5.png\n",
    "[image5A]: ./misc/thresh_rock.png\n",
    "[image6]: ./misc/perspective.png\n",
    "[image7]: ./misc/warp.png\n",
    "[image8]: ./misc/MapToWorld.png\n",
    "[image9]: ./misc/HSVthreshold.png\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## [Rubric](https://review.udacity.com/#!/rubrics/916/view) Points\n",
    "### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  \n",
    "\n",
    "---\n",
    "### Writeup / README\n",
    "\n",
    "\n",
    "#### Simulator / Environment Setup\n",
    "\n",
    "* MacBook Pro Running OS Sierra  10.12  \n",
    "* Installed  Anaconda and Jupyter Notebooks per the instructions.\n",
    "* used PIP to install moviepy \n",
    "* Downloaded MacOSX version Robo Simulator 2. The initial Robot Simulator didn't work well in automonous mode.\n",
    "* Forked a version of RoboND-Rover Rover to my  ejm2000 GitHub account.\n",
    "\n",
    "My Github account with the entire project and writeup:\n",
    "(https://github.com/ejm2000/RoboND-Rover-Project) \n",
    "         \n",
    "       \n",
    "        \n",
    "#### The Perception Step\n",
    "\n",
    "Modified a function to test  each channel R,G,B for a given pixel against a set RGB threhold. The color thresold function is set to (!60,160,160) which corresponds to the lighter color pixels which corresponds to navigable terrain.\n",
    "The code snippet and figures from before and after calling the color threshold function are below\n",
    "\n",
    "        def color_thresh(img, rgb_thresh=(160, 160, 160)):\n",
    "            # Create an array of zeros same xy size as img, but single channel\n",
    "            color_select = np.zeros_like(img[:,:,0])\n",
    "            # Require that each pixel be above all three threshold values in RGB\n",
    "            # above_thresh will now contain a boolean array with \"True\"\n",
    "            # where threshold was met\n",
    "            above_thresh = (img[:,:,0] > rgb_thresh[0]) \\\n",
    "                        & (img[:,:,1] > rgb_thresh[1]) \\\n",
    "                        & (img[:,:,2] > rgb_thresh[2])\n",
    "            # Index the array of zeros with the boolean array and set to 1\n",
    "            color_select[above_thresh] = 1\n",
    "            # Return the binary image\n",
    "            return color_select\n",
    "\n",
    "![Detectable Navigable Terrain before and after][image5]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "I wrote a custom the color ranging function to detect rock samples. The rock samples are yellow , so set range to detect rocks in that range.  Modifed the color_thresh function to take a upper and lower rnage of RBG values.\n",
    "\n",
    "       def color_range(img, rgb_thresh,object_thresh):\n",
    "            # Create an array of zeros same xy size as img, but single channel\n",
    "            color_select = np.zeros_like(img[:,:,0])\n",
    "            # Require that each pixel be above and below all three threshold values in RGB\n",
    "            # above_thresh & below_thresh will now contain a boolean array with \"True\"\n",
    "            # where threshold was met\n",
    "            above_thresh = (img[:,:,0] > rgb_thresh[0]) \\\n",
    "                    & (img[:,:,1] > rgb_thresh[1]) \\\n",
    "                    & (img[:,:,2] > rgb_thresh[2])\n",
    "\n",
    "            below_thresh = (img[:,:,0] < object_thresh[0]) \\\n",
    "                    & (img[:,:,1] < object_thresh[1]) \\\n",
    "                    & (img[:,:,2] < object_thresh[2])\n",
    "\n",
    "            # Index the array of zeros with the boolean array and set to 1 \n",
    "            # when the values are in range\n",
    "\n",
    "            color_select[(above_thresh & below_thresh)] = 1\n",
    "\n",
    "            # Return the binary image\n",
    "            return color_select                                     \n",
    "                            \n",
    "                            \n",
    "\n",
    "![Detectable Rocks][image5A]\n",
    "\n",
    "\n",
    "One  can also use openCv function called cvtColor  find upper and lower rangr of BGR  values and mask them to isolate the the target object. In the example below the function is used to find threshold for the rock samples.\n",
    "\n",
    "HSV colorspaces can be tricky to determine so I used the following utility to test out different color values\n",
    "* <http://www.rapidtables.com/web/color/color-picker.htm>\n",
    "\n",
    "The code and screenshots below demonstrates the process:\n",
    "\n",
    "        import cv2 # OpenCV for perspective transform\n",
    "        import numpy as np\n",
    "        example_rock = '../calibration_images/example_rock1.jpg'\n",
    "\n",
    "        rock_img = cv2.imread(example_rock)\n",
    "\n",
    "        # define range of yellow color in HSV\n",
    "        hsv = cv2.cvtColor(rock_img, cv2.COLOR_BGR2HSV)\n",
    "        lower_yellow = np.array([0,100,100])\n",
    "        upper_yellow = np.array([176,255,255])\n",
    "\n",
    "        # Threshold the HSV image to get only yellow colors\n",
    "        mask = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "        # Bitwise-AND mask and original image\n",
    "        result = cv2.bitwise_and(rock_img,hsv, mask= mask)\n",
    "\n",
    "\n",
    "        cv2.imshow('camera image of rock',rock_img)\n",
    "        cv2.imshow('mask',mask)\n",
    "        cv2.imshow('result',result)\n",
    "\n",
    "\n",
    "        cv2.waitKey(30000)\n",
    "        cv2.destroyAllWindows()                            \n",
    "\n",
    "\n",
    "![Using HSV color spaces to detect objects][image9]\n",
    "\n",
    "\n",
    "#### Perspective Transform\n",
    "\n",
    "Applied OpenCv functions to incoming Camera Image to convert perspective view to overhead view for mapping navigable terrain. Chose pixels from a perspective camera image blockand mapped them the associated pixels in an overhead view. \n",
    "\n",
    "Code excerpt:\n",
    "\n",
    "    source = np.float32([[10,140  ], [118,95], [200,95 ], [ 300,140]])\n",
    "    destination = np.float32([[ 150,150 ], [150 ,140], [160,140 ], [160,150 ]])      \n",
    "    warped = perspect_transform(image, source, destination)\n",
    "\n",
    "\n",
    "    def perspect_transform(img, src, dst):\n",
    "\n",
    "        # Get transform matrix using cv2.getPerspectivTransform()\n",
    "        M = cv2.getPerspectiveTransform(src, dst)\n",
    "        # Warp image using cv2.warpPerspective()\n",
    "        # keep same size as input image\n",
    "        warped = cv2.warpPerspective(img, M, (img.shape[1], img.shape[0]))\n",
    "        # Return the result\n",
    "        return warped\n",
    "    \n",
    "![Perspective][image6]\n",
    "\n",
    "#### Warp, Threshold\n",
    "\n",
    "Converted Map to Rover Centric Coordinates where the Camera is set to (0,0) on the coorindate plane\n",
    "\n",
    "    def rover_coords(binary_img):\n",
    "        # Calculate pixel positions with reference to the rover \n",
    "        # position being at the center bottom of the image.\n",
    "        ypos, xpos = colorsel.nonzero()\n",
    "        y_pixel = -(ypos - binary_img.shape[0]).astype(np.float)\n",
    "        x_pixel = -(xpos - binary_img.shape[1]/2).astype(np.float)\n",
    "        return x_pixel, y_pixel\n",
    "        \n",
    "![Rover Coordinates][image7]        \n",
    "\n",
    "####  Map to World Coordinates\n",
    "\n",
    "Applied a Rotational Matrix on the Rover's Position to get the yaw, then scaled & mapped it to the world coordinators.\n",
    "\n",
    "    # Define a function to apply a rotation to pixel positions\n",
    "    def rotate_pix(xpix, ypix, yaw):\n",
    "        # TODO:Done\n",
    "        # Convert yaw to radians\n",
    "        # Apply a rotation ,please\n",
    "        yaw_rad = yaw * np.pi / 180\n",
    "        xpix_rotated = xpix * np.cos(yaw_rad) - ypix * np.sin(yaw_rad)\n",
    "        ypix_rotated = xpix * np.sin(yaw_rad) + ypix * np.cos(yaw_rad)\n",
    "        # Return the rotated result  \n",
    "        return xpix_rotated, ypix_rotated\n",
    "\n",
    "    # Define a function to perform a translation\n",
    "    def translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale): \n",
    "        # TODO:Done\n",
    "        # Apply a scaling and a translation\n",
    "        # Perform translation and convert to integer since pixel values can't be float\n",
    "        xpix_translated = np.int_(xpos + (xpix_rot / scale))\n",
    "        ypix_translated = np.int_(ypos + (ypix_rot / scale))\n",
    "        # Return the result  \n",
    "        return xpix_translated, ypix_translated\n",
    "\n",
    "    def pix_to_world(xpix, ypix, xpos, ypos, yaw, world_size, scale):\n",
    "        # Apply rotation\n",
    "        xpix_rot, ypix_rot = rotate_pix(xpix, ypix, yaw)\n",
    "        # Apply translation\n",
    "        xpix_tran, ypix_tran = translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale)\n",
    "        # Perform rotation, translation and clipping all at once\n",
    "        x_pix_world = np.clip(np.int_(xpix_tran), 0, world_size - 1)\n",
    "        y_pix_world = np.clip(np.int_(ypix_tran), 0, world_size - 1)\n",
    "        # Return the result\n",
    "        return x_pix_world, y_pix_world\n",
    "        \n",
    "![Map to Word][image8]        \n",
    "\n",
    "#### Decisions and Taking Action\n",
    "\n",
    "Convert the x,y coordinates to polar coordinates to calculate the average angle to steer the rover.\n",
    "\n",
    "    Rover.nav_dists, Rover.nav_angles = to_polar_coords(xpix, ypix)\n",
    "    \n",
    "    def to_polar_coords(x_pixel, y_pixel):\n",
    "        # Convert (x_pixel, y_pixel) to (distance, angle) \n",
    "        # in polar coordinates in rover space\n",
    "        # Calculate distance to each pixel\n",
    "        dist = np.sqrt(x_pixel**2 + y_pixel**2)\n",
    "        # Calculate angle away from vertical for each pixel\n",
    "        angles = np.arctan2(y_pixel, x_pixel)\n",
    "        return dist, angles\n",
    "\n",
    "\n",
    "The Rover State is maintained in the Rover class. This class has attributes such as x,y position, speed, velocity, incoming camera images, yaw, etc.\n",
    "\n",
    "This information will serve as the basis for Rover control , decision making , and navigation.\n",
    "\n",
    "\n",
    "### Notebook Analysis\n",
    "#### 1. Run the functions provided in the notebook on test images (first with the test data provided, next on data you have recorded). Add/modify functions to allow for color selection of obstacles and rock samples. Describe in your writeup (and identify where in your code) how you modified or added functions to add obstacle and rock sample identification.\n",
    "\n",
    "Link to Rover Project Test NoteBook:\n",
    "* <http://localhost:8888/notebooks/code/Rover_Project_Test_Notebook.ipynb>\n",
    "\n",
    " \n",
    "I created a color_range thresholding function to detect rocks witin a certain color range. The sample rocks were in the yellow range of colors. It is similar to the function color_thresh which detects colors above a certain threshold\n",
    "\n",
    "\n",
    "                # Identify pixels above the threshold\n",
    "                # Threshold of RGB > 160 does a nice job of identifying ground pixels only\n",
    "                # Yellow is 255,255,0\n",
    "                # \n",
    "                def color_thresh(img, rgb_thresh=(160, 160, 160)):\n",
    "                    # Create an array of zeros same xy size as img, but single channel\n",
    "                    color_select = np.zeros_like(img[:,:,0])\n",
    "                    # Require that each pixel be above all three threshold values in RGB\n",
    "                    # above_thresh will now contain a boolean array with \"True\"\n",
    "                    # where threshold was met\n",
    "                    above_thresh = (img[:,:,0] > rgb_thresh[0]) \\\n",
    "                                & (img[:,:,1] > rgb_thresh[1]) \\\n",
    "                                & (img[:,:,2] > rgb_thresh[2])\n",
    "                    # Index the array of zeros with the boolean array and set to 1\n",
    "                    color_select[above_thresh] = 1\n",
    "                    # Return the binary image\n",
    "                    return color_select\n",
    "\n",
    "\n",
    "                def color_range(img, rgb_thresh,object_thresh):\n",
    "                    # Create an array of zeros same xy size as img, but single channel\n",
    "                    color_select = np.zeros_like(img[:,:,0])\n",
    "                    # Require that each pixel be above and below all three threshold values in RGB\n",
    "                    # above_thresh & below_thresh will now contain a boolean array with \"True\"\n",
    "                    # where threshold was met\n",
    "                    above_thresh = (img[:,:,0] > rgb_thresh[0]) \\\n",
    "                                & (img[:,:,1] > rgb_thresh[1]) \\\n",
    "                                & (img[:,:,2] > rgb_thresh[2])\n",
    "\n",
    "                    below_thresh = (img[:,:,0] < object_thresh[0]) \\\n",
    "                                & (img[:,:,1] < object_thresh[1]) \\\n",
    "                                & (img[:,:,2] < object_thresh[2])\n",
    "                    # Index the array of zeros with the boolean array and set to 1 when the values are in range\n",
    "                    color_select[(above_thresh & below_thresh)] = 1\n",
    "                    # Return the binary image\n",
    "                    return color_select\n",
    "\n",
    "                threshed = color_thresh(warped)\n",
    "                fig = plt.figure(figsize=(12,3))\n",
    "                plt.subplot(121)\n",
    "                plt.imshow(threshed, cmap='gray')\n",
    "\n",
    "                #Plot Object \n",
    "                #rgb_thresh = (160,160,160)  #navigable terrain\n",
    "                #object_thresh = (255,255,255)\n",
    "                #works for detecting just rocks\n",
    "                rgb_thresh = (170,134,0)  #rocks\n",
    "                object_thresh = (255,170,70)\n",
    "                threshed_rock = color_range(rock_warped,rgb_thresh,object_thresh)\n",
    "                plt.subplot(122)\n",
    "                plt.imshow(threshed_rock, cmap='gray')\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Populate the `process_image()` function with the appropriate analysis steps to map pixels identifying navigable terrain, obstacles and rock samples into a worldmap.  Run `process_image()` on your test data using the `moviepy` functions provided to create video output of your result. Describe in your writeup how you modified the process_image() to demonstrate your analysis and how you created a worldmap. Include your video output with your submission.\n",
    "\n",
    "Link to Rover Project Test NoteBook:\n",
    "* <http://localhost:8888/notebooks/code/Rover_Project_Test_Notebook.ipynb>\n",
    "\n",
    "---\n",
    "# The movie output of this code included in the link below:\n",
    "# * <http://localhost:8888/notebooks/output/test_mapping.mp4>\n",
    "\n",
    "---\n",
    "    \n",
    "Modified the calls to color_threshold & color_range functions to determine navigable objects, rocks and obstacles. \n",
    "As suggested in the Hints section, used an inverted navigable image to determine location of obstacles.\n",
    "\n",
    "Create function to map rover coordinators to map coordinates, and called the \"get_world_coordinates\" function for navigable , rock, and obstacle coordinates and mapped them to the data objects for proper display in the movie. An excerpt from the code is shown below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def get_world_coordinates(threshed):\n",
    "            scale = 10\n",
    "            # 4) Convert thresholded image pixel values to rover-centric coords\n",
    "            xpix, ypix = rover_coords(threshed)\n",
    "            # 5) Convert rover-centric pixel values to world coords\n",
    "\n",
    "            x_world, y_world = pix_to_world(xpix, ypix, np.float32(data.xpos[data.count]), \n",
    "                                        np.float32(data.ypos[data.count]), np.float32(data.yaw[data.count]), \n",
    "                                        data.worldmap.shape[0], scale)\n",
    "\n",
    "            return (x_world,y_world)\n",
    "\n",
    "        # Define a function to pass stored images to\n",
    "        # reading rover position and yaw angle from csv file\n",
    "        # This function will be used by moviepy to create an output video\n",
    "        def process_image(img):\n",
    "            # Example of how to use the Databucket() object defined above\n",
    "            # to print the current x, y and yaw values \n",
    "            # print (\"Data xpos,ypos,yaw\")\n",
    "            # print(data.xpos[data.count], data.ypos[data.count], data.yaw[data.count])\n",
    "\n",
    "            # TODO: Done\n",
    "            # 1) Define source and destination points for perspective transform\n",
    "            dst_size = 5 \n",
    "            bottom_offset = 6\n",
    "            scale = 10\n",
    "            source = np.float32([[14, 140], [301 ,140],[200, 96], [118, 96]])\n",
    "\n",
    "            destination = np.float32([[img.shape[1]/2 - dst_size, img.shape[0] - bottom_offset],\n",
    "                          [img.shape[1]/2 + dst_size, img.shape[0] - bottom_offset],\n",
    "                          [img.shape[1]/2 + dst_size, img.shape[0] - 2*dst_size - bottom_offset], \n",
    "                          [img.shape[1]/2 - dst_size, img.shape[0] - 2*dst_size - bottom_offset],\n",
    "                          ])\n",
    "            # 2) Apply perspective transform\n",
    "            warped = perspect_transform(img, source, destination)\n",
    "\n",
    "            # 3) Apply color thresholds to identify navigable terrain/obstacles/rock samples\n",
    "            threshed = color_thresh(warped)\n",
    "            threshed_obstacle = np.invert(threshed)  #inverse of navigable path are obstables\n",
    "\n",
    "            rgb_thresh = (170,134,0)  #rocks\n",
    "            object_thresh = (255,170,70)\n",
    "            threshed_rock = color_range(warped,rgb_thresh,object_thresh)\n",
    "\n",
    "            # 4) Convert thresholded image pixel values to rover-centric coords\n",
    "            # 5) Convert rover-centric pixel values to world coords\n",
    "            navigable_x_world,navigable_y_world = get_world_coordinates(threshed)\n",
    "            rock_x_world,rock_y_world = get_world_coordinates(threshed_rock)\n",
    "            obstacle_x_world,obstacle_y_world = get_world_coordinates(threshed_obstacle)\n",
    "\n",
    "\n",
    "\n",
    "            # 6) Update worldmap (to be displayed on right side of screen)\n",
    "                # Example: data.worldmap[obstacle_y_world, obstacle_x_world, 0] += 1\n",
    "                #          data.worldmap[rock_y_world, rock_x_world, 1] += 1\n",
    "                #          data.worldmap[navigable_y_world, navigable_x_world, 2] += 1\n",
    "\n",
    "            #Navigable World\n",
    "            data.worldmap[obstacle_y_world, obstacle_x_world, 0] += 1\n",
    "            data.worldmap[rock_y_world, rock_x_world, 1] += 1\n",
    "            data.worldmap[navigable_y_world, navigable_x_world, 2] += 1\n",
    "\n",
    "\n",
    "\n",
    "![alt text][image2]\n",
    "\n",
    "### Autonomous Navigation and Mapping\n",
    "\n",
    "#### 1. Fill in the `perception_step()` (at the bottom of the `perception.py` script) and `decision_step()` (in `decision.py`) functions in the autonomous mapping scripts and an explanation is provided in the writeup of how and why these functions were modified as they were.\n",
    "\n",
    "### perception_step() and decision_step() functions have been filled in and their functionality explained in the writeup.\n",
    "\n",
    "The perception_step was completed in a similar manner as the process_image() function described in the NoteBook Analysis. The primary difference being that  outputs were mapped to the Rover State Object. In addition the Rover nav_distance and angles were updated using the  \"to_polar_coordinates\" function\n",
    "\n",
    "        def get_world_coordinates(threshed,Rover):\n",
    "            scale = 10\n",
    "            # 5) Convert map image pixel values to rover-centric coords\n",
    "            xpix, ypix = rover_coords(threshed)    \n",
    "            # 6) Convert rover-centric pixel values to world coordinates\n",
    "\n",
    "            x_world, y_world = pix_to_world(xpix, ypix, np.float32(Rover.pos[0]), \n",
    "                                        np.float32(Rover.pos[1]), np.float32(Rover.yaw), \n",
    "                                        Rover.worldmap.shape[0], scale)\n",
    "\n",
    "            return (x_world,y_world)\n",
    "\n",
    "\n",
    "\n",
    "        # Apply the above functions in succession and update the Rover state accordingly\n",
    "        def perception_step(Rover):\n",
    "            # Perform perception steps to update Rover()\n",
    "            # TODO: Done\n",
    "            # NOTE: camera image is coming to you in Rover.img\n",
    "            # 1) Define source and destination points for perspective transform\n",
    "            dst_size = 5 \n",
    "            bottom_offset = 6\n",
    "            scale = 10\n",
    "            source = np.float32([[14, 140], [301 ,140],[200, 96], [118, 96]])\n",
    "            destination = np.float32([[Rover.img.shape[1]/2 - dst_size, Rover.img.shape[0] - bottom_offset],\n",
    "                          [Rover.img.shape[1]/2 + dst_size, Rover.img.shape[0] - bottom_offset],\n",
    "                          [Rover.img.shape[1]/2 + dst_size, Rover.img.shape[0] - 2*dst_size - bottom_offset], \n",
    "                          [Rover.img.shape[1]/2 - dst_size, Rover.img.shape[0] - 2*dst_size - bottom_offset],\n",
    "                          ])\n",
    "            # 2) Apply perspective transform\n",
    "            warped = perspect_transform(Rover.img, source, destination)\n",
    "            # 3) Apply color threshold to identify navigable terrain/obstacles/rock samples\n",
    "            threshed = color_thresh(warped)\n",
    "\n",
    "            threshed_obstacle = np.invert(threshed)  #inverse of navigable path are obstables\n",
    "\n",
    "            rgb_thresh = (170,134,0)  #rocks\n",
    "            object_thresh = (255,170,70)\n",
    "            threshed_rock = color_range(warped,rgb_thresh,object_thresh)\n",
    "\n",
    "\n",
    "            # 4) Update Rover.vision_image (this will be displayed on left side of screen)\n",
    "                # Example: Rover.vision_image[:,:,0] = obstacle color-thresholded binary image\n",
    "                #          Rover.vision_image[:,:,1] = rock_sample color-thresholded binary image\n",
    "                #          Rover.vision_image[:,:,2] = navigable terrain color-thresholded binary image\n",
    "\n",
    "            Rover.vision_image[:,:,0] = threshed_obstacle\n",
    "            Rover.vision_image[:,:,1] = threshed_rock\n",
    "            Rover.vision_image[:,:,2] = threshed*255\n",
    "\n",
    "            # 5) Convert map image pixel values to rover-centric coords\n",
    "            # 6) Convert rover-centric pixel values to world coordinates\n",
    "            navigable_x_world,navigable_y_world = get_world_coordinates(threshed,Rover)\n",
    "            rock_x_world,rock_y_world = get_world_coordinates(threshed_rock,Rover)\n",
    "            obstacle_x_world,obstacle_y_world = get_world_coordinates(threshed_obstacle,Rover)\n",
    "\n",
    "\n",
    "            # 7) Update Rover worldmap (to be displayed on right side of screen)\n",
    "                # Example: Rover.worldmap[obstacle_y_world, obstacle_x_world, 0] += 1\n",
    "                #          Rover.worldmap[rock_y_world, rock_x_world, 1] += 1\n",
    "                #          Rover.worldmap[navigable_y_world, navigable_x_world, 2] += 1\n",
    "\n",
    "            Rover.worldmap[obstacle_y_world, obstacle_x_world, 0] += 1\n",
    "            Rover.worldmap[rock_y_world, rock_x_world, 1] += 1\n",
    "            Rover.worldmap[navigable_y_world, navigable_x_world, 2] += 1\n",
    "\n",
    "            # 8) Convert rover-centric pixel positions to polar coordinates\n",
    "            # Update Rover pixel distances and angles\n",
    "                # Rover.nav_dists = rover_centric_pixel_distances\n",
    "                # Rover.nav_angles = rover_centric_angles\n",
    "\n",
    "            xpix, ypix = rover_coords(threshed)\n",
    "            Rover.nav_dists, Rover.nav_angles = to_polar_coords(xpix,ypix)\n",
    "\n",
    "\n",
    "            return Rover\n",
    "\n",
    "These nav angles were fed into the Decision_step\n",
    "\n",
    "\n",
    "#### 2. Launching in autonomous mode your rover can navigate and map autonomously.  By running drive_rover.py and launching the simulator in autonomous mode, your rover does a reasonably good job at mapping the environment. Explain your results and how you might improve them in your writeup.  \n",
    "\n",
    "#### The rover must map at least 40% of the environment with 60% fidelity (accuracy) against the ground truth. You must also find (map) the location of at least one rock sample. They don't need to pick any rocks up, just have them appear in the map (should happen automatically if their map pixels in Rover.worldmap[:,:,1] overlap with sample locations.)\n",
    "\n",
    "                       \n",
    " Here I'll talk about the approach I took, what techniques I used, what worked and why, where the pipeline might fail and how I might improve it if I were going to pursue this project further.  \n",
    " \n",
    "Simulator Settings:\n",
    "* Screen resolution set to 1280 x 800. Graphics Quality settings: Good\n",
    "               \n",
    " \n",
    "With the basic perception_step modifications described in Step #1, the rover mapped pretty well but tended to crash into obstacles that were located the center of the path. \n",
    "\n",
    "After updating it with more obstacle mapping information \n",
    "* Performance improved with 68% mapping and 67% fidelity in 200+ seconds.\n",
    "\n",
    "Autonomous Mapping ScreenShot from Simulator:\n",
    "![Autonomous Mapping ScreenShot][image4] \n",
    "\n",
    "\n",
    "\n",
    "Code Snippet from perception.py:\n",
    "\n",
    "            navigable_x_world,navigable_y_world = get_world_coordinates(threshed,Rover)\n",
    "            rock_x_world,rock_y_world = get_world_coordinates(threshed_rock,Rover)\n",
    "            obstacle_x_world,obstacle_y_world = get_world_coordinates(threshed_obstacle,Rover)\n",
    "\n",
    "            # 7) Update Rover worldmap (to be displayed on right side of screen)\n",
    "                \n",
    "            Rover.worldmap[obstacle_y_world, obstacle_x_world, 0] += 1\n",
    "            Rover.worldmap[rock_y_world, rock_x_world, 1] += 1\n",
    "            Rover.worldmap[navigable_y_world, navigable_x_world, 2] += 1\n",
    "\n",
    "            # 8) Convert rover-centric pixel positions to polar coordinates              \n",
    "            xpix, ypix = rover_coords(threshed)\n",
    "            Rover.nav_dists, Rover.nav_angles = to_polar_coords(xpix,ypix)                           \n",
    "\n",
    "\n",
    "\n",
    "Unfortunately time did not allow for me to try the activities listed in challenge.\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
