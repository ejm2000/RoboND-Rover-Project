{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Search and Sample Return\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**The goals / steps of this project are the following:**  \n",
    "\n",
    "**Training / Calibration**  \n",
    "\n",
    "* Download the simulator and take data in \"Training Mode\"\n",
    "* Test out the functions in the Jupyter Notebook provided\n",
    "* Add functions to detect obstacles and samples of interest (golden rocks)\n",
    "* Fill in the `process_image()` function with the appropriate image processing steps (perspective transform, color threshold etc.) to get from raw images to a map.  The `output_image` you create in this step should demonstrate that your mapping pipeline works.\n",
    "* Use `moviepy` to process the images in your saved dataset with the `process_image()` function.  Include the video you produce as part of your submission.\n",
    "\n",
    "**Autonomous Navigation / Mapping**\n",
    "\n",
    "* Fill in the `perception_step()` function within the `perception.py` script with the appropriate image processing functions to create a map and update `Rover()` data (similar to what you did with `process_image()` in the notebook). \n",
    "* Fill in the `decision_step()` function within the `decision.py` script with conditional statements that take into consideration the outputs of the `perception_step()` in deciding how to issue throttle, brake and steering commands. \n",
    "* Iterate on your perception and decision function until your rover does a reasonable (need to define metric) job of navigating and mapping.  \n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./misc/rover_image.jpg\n",
    "[image2]: ./calibration_images/example_grid1.jpg\n",
    "[image3]: ./calibration_images/example_rock1.jpg \n",
    "\n",
    "## [Rubric](https://review.udacity.com/#!/rubrics/916/view) Points\n",
    "### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  \n",
    "\n",
    "---\n",
    "### Writeup / README\n",
    "\n",
    "\n",
    "* Simulator / Environment Setup\n",
    "\n",
    "- MacBook Pro Running OS Sierra  10.12  \n",
    "- Installed  Anaconda and Jupyter Notebooks per the instructions.\n",
    "- used PIP to install moviepy \n",
    "- Downloaded MacOSX version Robo Simulator 2. The initial Robot Simulator didn't work well in automonous mode.\n",
    "- Forked a version of RoboND-Rover Rover to my  ejm2000 GitHub account.\n",
    "        https://github.com/ejm2000/RoboND-Rover-Project\n",
    "        \n",
    "        \n",
    "* The Perception Step\n",
    "\n",
    "Modified a function to test  each channel R,G,B for a given  pixel against a set RGB threhold \n",
    "-- 160,160,160 for the background color\n",
    "-- 112,112,112 for the rocks to \n",
    "-- 50,50,50 for the obstacles\n",
    "\n",
    "* Perspective Transform\n",
    "\n",
    "* Warp, Threshold\n",
    "\n",
    "* Map to World Coordinates\n",
    "\n",
    "* Decisions and Taking Action\n",
    "\n",
    "\n",
    "### Notebook Analysis\n",
    "#### 1. Run the functions provided in the notebook on test images (first with the test data provided, next on data you have recorded). Add/modify functions to allow for color selection of obstacles and rock samples.\n",
    "\n",
    "From DataSet\n",
    "\n",
    "![alt text][image1]\n",
    "![alt text][image]\n",
    "\n",
    "\n",
    "From Manual Runs of the Simulator\n",
    "\n",
    "#### 1. Populate the `process_image()` function with the appropriate analysis steps to map pixels identifying navigable terrain, obstacles and rock samples into a worldmap.  Run `process_image()` on your test data using the `moviepy` functions provided to create video output of your result. \n",
    "And another! \n",
    "\n",
    "![alt text][image2]\n",
    "### Autonomous Navigation and Mapping\n",
    "\n",
    "#### 1. Fill in the `perception_step()` (at the bottom of the `perception.py` script) and `decision_step()` (in `decision.py`) functions in the autonomous mapping scripts and an explanation is provided in the writeup of how and why these functions were modified as they were.\n",
    "\n",
    "\n",
    "#### 2. Launching in autonomous mode your rover can navigate and map autonomously.  Explain your results and how you might improve them in your writeup.  \n",
    "\n",
    "**Note: running the simulator with different choices of resolution and graphics quality may produce different results, particularly on different machines!  Make a note of your simulator settings (resolution and graphics quality set on launch) and frames per second (FPS output to terminal by `drive_rover.py`) in your writeup when you submit the project so your reviewer can reproduce your results.**\n",
    "\n",
    "Here I'll talk about the approach I took, what techniques I used, what worked and why, where the pipeline might fail and how I might improve it if I were going to pursue this project further.  \n",
    "\n",
    "\n",
    "\n",
    "![alt text][image3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
